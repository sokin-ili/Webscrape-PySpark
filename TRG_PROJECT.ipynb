{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "051c646a",
   "metadata": {},
   "source": [
    "### install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582e9832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38fe8ff",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1917ffad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:09:11.338394Z",
     "start_time": "2022-07-03T21:09:07.338392Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "#import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "066c62f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:09:12.413476Z",
     "start_time": "2022-07-03T21:09:12.246918Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56ed6c5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:09:13.646833Z",
     "start_time": "2022-07-03T21:09:13.629213Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3884caf8",
   "metadata": {},
   "source": [
    "### url link\n",
    "#### check response = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a787e965",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:09:17.811112Z",
     "start_time": "2022-07-03T21:09:17.343041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.atptour.com/en/rankings/singles?rankRange=0-100&rankDate=2022-06-27\"\n",
    "headers={'User-Agent': ''}\n",
    "page = requests.get(url,timeout=15, headers= headers)\n",
    "print(page)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f29ea9",
   "metadata": {},
   "source": [
    "### main ranking table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af0f185b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:09:24.406258Z",
     "start_time": "2022-07-03T21:09:23.859103Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "rows = soup.find(\"div\", {\"class\": \"table-rankings-wrapper\"}).find_all('tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec19652e",
   "metadata": {},
   "source": [
    "### 1. ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6afdc786",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:09:25.767996Z",
     "start_time": "2022-07-03T21:09:25.745844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking = rows[1].find('td', {\"class\":\"rank-cell\"}).get_text().strip()\n",
    "ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea1fbb",
   "metadata": {},
   "source": [
    "### 2. country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab7e273d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:09:26.509488Z",
     "start_time": "2022-07-03T21:09:26.496466Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RUS'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country = rows[1].find(\"div\", {\"class\": \"country-item\"}).find(\"img\").get('alt')\n",
    "country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b008b0f",
   "metadata": {},
   "source": [
    "### 3. name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891633e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:09:26.844854Z",
     "start_time": "2022-07-03T21:09:26.827048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Daniil Medvedev'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = rows[1].find(\"a\", {\"class\": \"\"}).get_text().strip()\n",
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d003f4",
   "metadata": {},
   "source": [
    "### 4. age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77808611",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:09:27.181344Z",
     "start_time": "2022-07-03T21:09:27.164549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'26'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age = rows[1].find('td', {\"class\": \"age-cell\"}).get_text().strip()\n",
    "age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6503669",
   "metadata": {},
   "source": [
    "### 5. points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "949d93c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:09:27.503053Z",
     "start_time": "2022-07-03T21:09:27.490090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7,955'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = rows[1].find('td', {\"class\":\"points-cell\"}).get_text().strip()\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a362a6",
   "metadata": {},
   "source": [
    "### 6. tournament played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f92b5e35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:09:28.462304Z",
     "start_time": "2022-07-03T21:09:28.444741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'21'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tournaments = rows[1].find('td', {'class':\"tourn-cell\"}).get_text().strip()\n",
    "tournaments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd45361",
   "metadata": {},
   "source": [
    "### 7. date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77bacb5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:09:45.750476Z",
     "start_time": "2022-07-03T21:09:45.734276Z"
    }
   },
   "outputs": [],
   "source": [
    "date = soup.find('div', class_= 'dropdown-layout-wrapper rank-detail-filter').find_all('ul')[2].find_all(\"li\")[1].get_text().strip().replace(\".\", \"-\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc12639a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:09:46.407419Z",
     "start_time": "2022-07-03T21:09:46.394261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-06-27'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b24324",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-01T18:03:45.501059Z",
     "start_time": "2022-07-01T18:03:45.487097Z"
    }
   },
   "outputs": [],
   "source": [
    "#nr_rows = len(soup.find('table', class_=\"mega-table\" ).find('tbody').find_all('tr'))\n",
    "#nr_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef0ac6c",
   "metadata": {},
   "source": [
    "### first row of most recent date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9373aba2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:09:52.084580Z",
     "start_time": "2022-07-03T21:09:52.059449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ranking</th>\n",
       "      <th>country</th>\n",
       "      <th>player</th>\n",
       "      <th>age</th>\n",
       "      <th>points</th>\n",
       "      <th>tournaments</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>RUS</td>\n",
       "      <td>Daniil Medvedev</td>\n",
       "      <td>26</td>\n",
       "      <td>7,955</td>\n",
       "      <td>21</td>\n",
       "      <td>2022-06-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ranking country           player age points tournaments        date\n",
       "0       1     RUS  Daniil Medvedev  26  7,955          21  2022-06-27"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([{\n",
    "    \"ranking\": ranking,\n",
    "    \"country\": country,\n",
    "    \"player\": name,\n",
    "    \"age\": age,\n",
    "    \"points\": points,\n",
    "    \"tournaments\": tournaments,\n",
    "    \"date\": date\n",
    "}])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503a85aa",
   "metadata": {},
   "source": [
    "### create a list with dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07edd08a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:09:57.201377Z",
     "start_time": "2022-07-03T21:09:57.174527Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_down=soup.find('div', {'class': 'main-content'}).find('ul',{'class': 'dropdown'})\n",
    "#first one repeats so removing 0th element\n",
    "date_list=[]\n",
    "\n",
    "dates =soup.find('div', class_= 'dropdown-layout-wrapper rank-detail-filter').find_all('ul')[2].find_all(\"li\")\n",
    "\n",
    "for x in dates[1:]:\n",
    "    date_list.append(x.get('data-value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f7c66e",
   "metadata": {},
   "source": [
    "### grab last 2 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "984f353a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:10:07.094138Z",
     "start_time": "2022-07-03T21:10:07.073992Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2022-06-27',\n",
       " '2022-06-20',\n",
       " '2022-06-13',\n",
       " '2022-06-06',\n",
       " '2022-05-23',\n",
       " '2022-05-16',\n",
       " '2022-05-09',\n",
       " '2022-05-02',\n",
       " '2022-04-25',\n",
       " '2022-04-18',\n",
       " '2022-04-11',\n",
       " '2022-04-04',\n",
       " '2022-03-21',\n",
       " '2022-03-07',\n",
       " '2022-02-28',\n",
       " '2022-02-21',\n",
       " '2022-02-14',\n",
       " '2022-02-07',\n",
       " '2022-01-31',\n",
       " '2022-01-17',\n",
       " '2022-01-10',\n",
       " '2022-01-03',\n",
       " '2021-12-27',\n",
       " '2021-12-20',\n",
       " '2021-12-13',\n",
       " '2021-12-06',\n",
       " '2021-11-29',\n",
       " '2021-11-22',\n",
       " '2021-11-15',\n",
       " '2021-11-08',\n",
       " '2021-11-01',\n",
       " '2021-10-25',\n",
       " '2021-10-18',\n",
       " '2021-10-04',\n",
       " '2021-09-27',\n",
       " '2021-09-20',\n",
       " '2021-09-13',\n",
       " '2021-08-30',\n",
       " '2021-08-23',\n",
       " '2021-08-16',\n",
       " '2021-08-09',\n",
       " '2021-08-02',\n",
       " '2021-07-26',\n",
       " '2021-07-19',\n",
       " '2021-07-12',\n",
       " '2021-06-28',\n",
       " '2021-06-21',\n",
       " '2021-06-14',\n",
       " '2021-05-31',\n",
       " '2021-05-24',\n",
       " '2021-05-17',\n",
       " '2021-05-10',\n",
       " '2021-05-03',\n",
       " '2021-04-26',\n",
       " '2021-04-19',\n",
       " '2021-04-12',\n",
       " '2021-04-05',\n",
       " '2021-03-22',\n",
       " '2021-03-15',\n",
       " '2021-03-08',\n",
       " '2021-03-01',\n",
       " '2021-02-22',\n",
       " '2021-02-08',\n",
       " '2021-02-01',\n",
       " '2021-01-25',\n",
       " '2021-01-18',\n",
       " '2021-01-11',\n",
       " '2021-01-04',\n",
       " '2020-12-28',\n",
       " '2020-12-21',\n",
       " '2020-12-14',\n",
       " '2020-12-07',\n",
       " '2020-11-30',\n",
       " '2020-11-23',\n",
       " '2020-11-16',\n",
       " '2020-11-09',\n",
       " '2020-11-02',\n",
       " '2020-10-26',\n",
       " '2020-10-19',\n",
       " '2020-10-12',\n",
       " '2020-09-28',\n",
       " '2020-09-21',\n",
       " '2020-09-14',\n",
       " '2020-08-31',\n",
       " '2020-08-24',\n",
       " '2020-03-16',\n",
       " '2020-03-09',\n",
       " '2020-03-02',\n",
       " '2020-02-24',\n",
       " '2020-02-17',\n",
       " '2020-02-10',\n",
       " '2020-02-03',\n",
       " '2020-01-20',\n",
       " '2020-01-13',\n",
       " '2020-01-06',\n",
       " '2019-12-30']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_list[:96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06da965c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:10:10.273959Z",
     "start_time": "2022-07-03T21:10:10.260592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-06-27'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc35f94",
   "metadata": {},
   "source": [
    "### scrape function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca592e2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:10:14.266836Z",
     "start_time": "2022-07-03T21:10:14.245199Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "def scrape(date):\n",
    "    urlpattern = \"https://www.atptour.com/en/rankings/singles?rankRange=0-100&rankDate={}\"\n",
    "    url = urlpattern.format(date)\n",
    "    headers={'User-Agent': ''}\n",
    "    response = requests.get(url,timeout=15, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        rows = soup.find(\"div\", {\"class\": \"table-rankings-wrapper\"}).find_all('tr') \n",
    "        lst = []\n",
    "        for row in rows[1:]:\n",
    "            \n",
    "            #Ranking\n",
    "            try:\n",
    "                ranking = row.find(\"td\", {\"class\": \"rank-cell\"}).get_text().strip()\n",
    "            except:\n",
    "                ranking = np.nan \n",
    "            \n",
    "            #Country\n",
    "            try:\n",
    "                country = row.find(\"div\", {\"class\": \"country-item\"}).find(\"img\")['alt']\n",
    "            except:\n",
    "                country = np.nan\n",
    "            \n",
    "            #Player\n",
    "            try:\n",
    "                player = row.find(\"td\", {\"class\": \"player-cell\"}).get_text().strip()\n",
    "            except:\n",
    "                player = np.nan\n",
    "            \n",
    "            #Age\n",
    "            try:\n",
    "                age = row.find(\"td\", {\"class\": \"age-cell\"}).get_text().strip()\n",
    "            except:\n",
    "                age = np.nan\n",
    "            \n",
    "            #Points\n",
    "            try:\n",
    "                points = row.find(\"a\", {\"ga-label\": \"rankings-breakdown\"}).get_text().strip()\n",
    "            except:\n",
    "                points = np.nan\n",
    "            \n",
    "            #Tournaments\n",
    "            try:\n",
    "                tournaments = row.find(\"td\", {\"class\": \"tourn-cell\"}).get_text().strip()\n",
    "            except:\n",
    "                tournaments = np.nan\n",
    "            \n",
    "            \n",
    "            temp = {\n",
    "                \"ranking\": ranking,\n",
    "                \"country\": country,\n",
    "                \"player\": player,\n",
    "                \"age\": age,\n",
    "                \"points\": points,\n",
    "                \"tournaments\": tournaments,\n",
    "                \"date\": date\n",
    "            }\n",
    "            lst.append(temp)\n",
    "            \n",
    "#             df = df.append(temp, ignore_index=True)\n",
    "                \n",
    "    else:\n",
    "        print('Scraper is down!')\n",
    "            \n",
    "    return pd.DataFrame(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b6e178",
   "metadata": {},
   "source": [
    "### scrape top100 of the most recent date (2022-06-27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d46c477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:10:22.954299Z",
     "start_time": "2022-07-03T21:10:21.945937Z"
    }
   },
   "outputs": [],
   "source": [
    "df = scrape(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecb93906",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:10:23.624242Z",
     "start_time": "2022-07-03T21:10:23.598960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ranking</th>\n",
       "      <th>country</th>\n",
       "      <th>player</th>\n",
       "      <th>age</th>\n",
       "      <th>points</th>\n",
       "      <th>tournaments</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>RUS</td>\n",
       "      <td>Daniil Medvedev</td>\n",
       "      <td>26</td>\n",
       "      <td>7,955</td>\n",
       "      <td>21</td>\n",
       "      <td>2022-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GER</td>\n",
       "      <td>Alexander Zverev</td>\n",
       "      <td>25</td>\n",
       "      <td>7,030</td>\n",
       "      <td>19</td>\n",
       "      <td>2022-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>SRB</td>\n",
       "      <td>Novak Djokovic</td>\n",
       "      <td>35</td>\n",
       "      <td>6,770</td>\n",
       "      <td>12</td>\n",
       "      <td>2022-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ESP</td>\n",
       "      <td>Rafael Nadal</td>\n",
       "      <td>36</td>\n",
       "      <td>6,525</td>\n",
       "      <td>10</td>\n",
       "      <td>2022-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>GRE</td>\n",
       "      <td>Stefanos Tsitsipas</td>\n",
       "      <td>23</td>\n",
       "      <td>5,150</td>\n",
       "      <td>25</td>\n",
       "      <td>2022-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>SUI</td>\n",
       "      <td>Henri Laaksonen</td>\n",
       "      <td>30</td>\n",
       "      <td>606</td>\n",
       "      <td>25</td>\n",
       "      <td>2022-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>SUI</td>\n",
       "      <td>Roger Federer</td>\n",
       "      <td>40</td>\n",
       "      <td>600</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>ESP</td>\n",
       "      <td>Carlos Taberner</td>\n",
       "      <td>24</td>\n",
       "      <td>585</td>\n",
       "      <td>35</td>\n",
       "      <td>2022-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>AUS</td>\n",
       "      <td>Jason Kubler</td>\n",
       "      <td>29</td>\n",
       "      <td>580</td>\n",
       "      <td>27</td>\n",
       "      <td>2022-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>ESP</td>\n",
       "      <td>Pablo Andujar</td>\n",
       "      <td>36</td>\n",
       "      <td>576</td>\n",
       "      <td>23</td>\n",
       "      <td>2022-06-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ranking country              player age points tournaments        date\n",
       "0        1     RUS     Daniil Medvedev  26  7,955          21  2022-06-27\n",
       "1        2     GER    Alexander Zverev  25  7,030          19  2022-06-27\n",
       "2        3     SRB      Novak Djokovic  35  6,770          12  2022-06-27\n",
       "3        4     ESP        Rafael Nadal  36  6,525          10  2022-06-27\n",
       "4        5     GRE  Stefanos Tsitsipas  23  5,150          25  2022-06-27\n",
       "..     ...     ...                 ...  ..    ...         ...         ...\n",
       "95      96     SUI     Henri Laaksonen  30    606          25  2022-06-27\n",
       "96      97     SUI       Roger Federer  40    600           3  2022-06-27\n",
       "97      98     ESP     Carlos Taberner  24    585          35  2022-06-27\n",
       "98      99     AUS        Jason Kubler  29    580          27  2022-06-27\n",
       "99     100     ESP       Pablo Andujar  36    576          23  2022-06-27\n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ee8b6",
   "metadata": {},
   "source": [
    "### scrape top 100 for dates:2019-2022\n",
    "#### this is the final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea151c8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:12:11.434122Z",
     "start_time": "2022-07-03T21:10:43.662218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479d826eb71c4ad79af5237e47b8ebe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_final = pd.DataFrame(columns=[\"ranking\", \"country\", \"player\", \"points\", \"tournaments\", \"date\"])\n",
    "dates_df = df['date'].unique()\n",
    "\n",
    "for date in tqdm(date_list[:96]):\n",
    "    if date not in dates_df:\n",
    "        temp_df = scrape(date)\n",
    "        df_final = pd.concat([df_final, temp_df],ignore_index=True)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0ac6ff3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:12:12.971129Z",
     "start_time": "2022-07-03T21:12:12.951730Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ranking</th>\n",
       "      <th>country</th>\n",
       "      <th>player</th>\n",
       "      <th>points</th>\n",
       "      <th>tournaments</th>\n",
       "      <th>date</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>RUS</td>\n",
       "      <td>Daniil Medvedev</td>\n",
       "      <td>8,160</td>\n",
       "      <td>21</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GER</td>\n",
       "      <td>Alexander Zverev</td>\n",
       "      <td>7,030</td>\n",
       "      <td>19</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>SRB</td>\n",
       "      <td>Novak Djokovic</td>\n",
       "      <td>6,770</td>\n",
       "      <td>12</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ESP</td>\n",
       "      <td>Rafael Nadal</td>\n",
       "      <td>6,525</td>\n",
       "      <td>10</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NOR</td>\n",
       "      <td>Casper Ruud</td>\n",
       "      <td>5,050</td>\n",
       "      <td>28</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9495</th>\n",
       "      <td>96</td>\n",
       "      <td>ITA</td>\n",
       "      <td>Salvatore Caruso</td>\n",
       "      <td>586</td>\n",
       "      <td>28</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9496</th>\n",
       "      <td>97</td>\n",
       "      <td>AUS</td>\n",
       "      <td>Alexei Popyrin</td>\n",
       "      <td>585</td>\n",
       "      <td>25</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9497</th>\n",
       "      <td>98</td>\n",
       "      <td>BLR</td>\n",
       "      <td>Egor Gerasimov</td>\n",
       "      <td>581</td>\n",
       "      <td>23</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9498</th>\n",
       "      <td>99</td>\n",
       "      <td>RSA</td>\n",
       "      <td>Lloyd Harris</td>\n",
       "      <td>576</td>\n",
       "      <td>24</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9499</th>\n",
       "      <td>100</td>\n",
       "      <td>AUS</td>\n",
       "      <td>James Duckworth</td>\n",
       "      <td>576</td>\n",
       "      <td>28</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ranking country            player points tournaments        date age\n",
       "0          1     RUS   Daniil Medvedev  8,160          21  2022-06-20  26\n",
       "1          2     GER  Alexander Zverev  7,030          19  2022-06-20  25\n",
       "2          3     SRB    Novak Djokovic  6,770          12  2022-06-20  35\n",
       "3          4     ESP      Rafael Nadal  6,525          10  2022-06-20  36\n",
       "4          5     NOR       Casper Ruud  5,050          28  2022-06-20  23\n",
       "...      ...     ...               ...    ...         ...         ...  ..\n",
       "9495      96     ITA  Salvatore Caruso    586          28  2019-12-30  27\n",
       "9496      97     AUS    Alexei Popyrin    585          25  2019-12-30  20\n",
       "9497      98     BLR    Egor Gerasimov    581          23  2019-12-30  27\n",
       "9498      99     RSA      Lloyd Harris    576          24  2019-12-30  22\n",
       "9499     100     AUS   James Duckworth    576          28  2019-12-30  27\n",
       "\n",
       "[9500 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53768351",
   "metadata": {},
   "source": [
    "### extract renkings to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d34a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T12:51:25.122055Z",
     "start_time": "2022-07-03T12:51:25.055609Z"
    }
   },
   "outputs": [],
   "source": [
    "df_final.to_csv(r'C:\\Users\\sokin\\Desktop\\atp-rankings.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5fff8",
   "metadata": {},
   "source": [
    "### PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "928912ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:13:24.651202Z",
     "start_time": "2022-07-03T21:13:13.386349Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('trg').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbdd2467",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:13:27.667309Z",
     "start_time": "2022-07-03T21:13:25.445765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.56.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>trg</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x202a1fb8940>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b956f2d7",
   "metadata": {},
   "source": [
    "### let's check the dataframe in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2bbd4fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:13:46.893144Z",
     "start_time": "2022-07-03T21:13:39.669772Z"
    }
   },
   "outputs": [],
   "source": [
    "df_spark = spark.read.csv('atp-rankings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01c88a95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:13:49.124914Z",
     "start_time": "2022-07-03T21:13:48.310443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+------+-----------+----------+---+\n",
      "|ranking|country|              player|points|tournaments|      date|age|\n",
      "+-------+-------+--------------------+------+-----------+----------+---+\n",
      "|      1|    RUS|     Daniil Medvedev| 8,160|         21|2022-06-20| 26|\n",
      "|      2|    GER|    Alexander Zverev| 7,030|         19|2022-06-20| 25|\n",
      "|      3|    SRB|      Novak Djokovic| 6,770|         12|2022-06-20| 35|\n",
      "|      4|    ESP|        Rafael Nadal| 6,525|         10|2022-06-20| 36|\n",
      "|      5|    NOR|         Casper Ruud| 5,050|         28|2022-06-20| 23|\n",
      "|      6|    GRE|  Stefanos Tsitsipas| 4,945|         24|2022-06-20| 23|\n",
      "|      7|    ESP|      Carlos Alcaraz| 4,893|         20|2022-06-20| 19|\n",
      "|      8|    RUS|       Andrey Rublev| 3,870|         26|2022-06-20| 24|\n",
      "|      9|    CAN|Felix Auger-Alias...| 3,760|         25|2022-06-20| 21|\n",
      "|     10|    POL|      Hubert Hurkacz| 3,738|         25|2022-06-20| 25|\n",
      "|     11|    ITA|   Matteo Berrettini| 3,480|         17|2022-06-20| 26|\n",
      "|     12|    GBR|      Cameron Norrie| 3,200|         27|2022-06-20| 26|\n",
      "|     13|    ITA|       Jannik Sinner| 3,185|         27|2022-06-20| 20|\n",
      "|     14|    USA|        Taylor Fritz| 2,920|         27|2022-06-20| 24|\n",
      "|     15|    ARG|   Diego Schwartzman| 2,325|         24|2022-06-20| 29|\n",
      "|     16|    CAN|    Denis Shapovalov| 2,293|         22|2022-06-20| 23|\n",
      "|     17|    CRO|         Marin Cilic| 2,220|         22|2022-06-20| 33|\n",
      "|     18|    USA|       Reilly Opelka| 2,100|         25|2022-06-20| 24|\n",
      "|     19|    ESP| Pablo Carreno Busta| 2,045|         25|2022-06-20| 30|\n",
      "|     20|    ESP|Roberto Bautista ...| 1,903|         23|2022-06-20| 34|\n",
      "+-------+-------+--------------------+------+-----------+----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.read.option('header', 'true').csv('atp-rankings.csv')\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd51e8fd",
   "metadata": {},
   "source": [
    "### some data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f8c21ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:14:11.167179Z",
     "start_time": "2022-07-03T21:14:11.157557Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1deff1f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:14:14.024121Z",
     "start_time": "2022-07-03T21:14:13.604269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+------+-----------+----------+---+\n",
      "|ranking|country|              player|points|tournaments|      date|age|\n",
      "+-------+-------+--------------------+------+-----------+----------+---+\n",
      "|      1|    RUS|     Daniil Medvedev|  8160|         21|2022-06-20| 26|\n",
      "|      2|    GER|    Alexander Zverev|  7030|         19|2022-06-20| 25|\n",
      "|      3|    SRB|      Novak Djokovic|  6770|         12|2022-06-20| 35|\n",
      "|      4|    ESP|        Rafael Nadal|  6525|         10|2022-06-20| 36|\n",
      "|      5|    NOR|         Casper Ruud|  5050|         28|2022-06-20| 23|\n",
      "|      6|    GRE|  Stefanos Tsitsipas|  4945|         24|2022-06-20| 23|\n",
      "|      7|    ESP|      Carlos Alcaraz|  4893|         20|2022-06-20| 19|\n",
      "|      8|    RUS|       Andrey Rublev|  3870|         26|2022-06-20| 24|\n",
      "|      9|    CAN|Felix Auger-Alias...|  3760|         25|2022-06-20| 21|\n",
      "|     10|    POL|      Hubert Hurkacz|  3738|         25|2022-06-20| 25|\n",
      "|     11|    ITA|   Matteo Berrettini|  3480|         17|2022-06-20| 26|\n",
      "|     12|    GBR|      Cameron Norrie|  3200|         27|2022-06-20| 26|\n",
      "|     13|    ITA|       Jannik Sinner|  3185|         27|2022-06-20| 20|\n",
      "|     14|    USA|        Taylor Fritz|  2920|         27|2022-06-20| 24|\n",
      "|     15|    ARG|   Diego Schwartzman|  2325|         24|2022-06-20| 29|\n",
      "|     16|    CAN|    Denis Shapovalov|  2293|         22|2022-06-20| 23|\n",
      "|     17|    CRO|         Marin Cilic|  2220|         22|2022-06-20| 33|\n",
      "|     18|    USA|       Reilly Opelka|  2100|         25|2022-06-20| 24|\n",
      "|     19|    ESP| Pablo Carreno Busta|  2045|         25|2022-06-20| 30|\n",
      "|     20|    ESP|Roberto Bautista ...|  1903|         23|2022-06-20| 34|\n",
      "+-------+-------+--------------------+------+-----------+----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = df_spark.withColumn('points', regexp_replace('points', ',', ''))\n",
    "df_spark = df_spark.withColumn(\"points\",df_spark[\"points\"].cast(IntegerType()))\n",
    "df_spark = df_spark.withColumn(\"tournaments\",df_spark[\"tournaments\"].cast(IntegerType()))\n",
    "df_spark = df_spark.withColumn(\"age\",df_spark[\"age\"].cast(IntegerType())) \n",
    "df_spark = df_spark.withColumn(\"ranking\",df_spark[\"ranking\"].cast(IntegerType())) \n",
    "\n",
    "df_spark.select('points', 'tournaments', 'age').dtypes\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f22143e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:14:21.581194Z",
     "start_time": "2022-07-03T21:14:21.543438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ranking', 'int'), ('points', 'int'), ('tournaments', 'int'), ('age', 'int')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.select('ranking', 'points', 'tournaments', \"age\").dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9c539c",
   "metadata": {},
   "source": [
    "## The goal is to create a streaming process that will read each input file(a dataframe per date) for every unique date. A transformation will be implemented in order to find the sum of points for every country\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eca432",
   "metadata": {},
   "source": [
    "###  sum of points for each country per date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bffe2cdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:15:18.629011Z",
     "start_time": "2022-07-03T21:15:17.739231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+\n",
      "|      date|country|sum(points)|\n",
      "+----------+-------+-----------+\n",
      "|2022-06-20|    FRA|       7423|\n",
      "|2022-06-20|    NED|       2477|\n",
      "|2022-06-20|    SWE|        659|\n",
      "|2022-06-20|    CZE|       1615|\n",
      "|2022-06-20|    BRA|        622|\n",
      "|2022-06-20|    NOR|       5050|\n",
      "|2022-06-20|    BLR|       1082|\n",
      "|2022-06-20|    TPE|        626|\n",
      "|2022-06-20|    FIN|       1015|\n",
      "|2022-06-20|    CAN|       6053|\n",
      "|2022-06-20|    HUN|        932|\n",
      "|2022-06-20|    BUL|       1785|\n",
      "|2022-06-20|    ESP|      21829|\n",
      "|2022-06-20|    RSA|       1015|\n",
      "|2022-06-20|    GRE|       4945|\n",
      "|2022-06-20|    ARG|       6800|\n",
      "|2022-06-20|    GER|       9693|\n",
      "|2022-06-20|    BEL|        930|\n",
      "|2022-06-20|    KOR|        772|\n",
      "|2022-06-20|    CRO|       2220|\n",
      "|2022-06-20|    BOL|        729|\n",
      "|2022-06-20|    POR|        918|\n",
      "|2022-06-20|    USA|      17105|\n",
      "|2022-06-20|    SVK|       1002|\n",
      "|2022-06-20|    GBR|       5396|\n",
      "|2022-06-20|    SUI|       1206|\n",
      "|2022-06-20|    POL|       4373|\n",
      "|2022-06-20|    DEN|       1429|\n",
      "|2022-06-20|    CHI|       1793|\n",
      "|2022-06-20|    RUS|      14841|\n",
      "|2022-06-20|    AUS|       6941|\n",
      "|2022-06-20|    GEO|       1473|\n",
      "|2022-06-20|    KAZ|       1105|\n",
      "|2022-06-20|    SRB|      11260|\n",
      "|2022-06-20|    ITA|       9578|\n",
      "|2022-06-13|    TPE|        608|\n",
      "|2022-06-13|    POL|       3973|\n",
      "|2022-06-13|    BOL|        732|\n",
      "|2022-06-13|    CRO|       2130|\n",
      "|2022-06-13|    SVK|       1046|\n",
      "|2022-06-13|    SUI|       1456|\n",
      "|2022-06-13|    RSA|       1105|\n",
      "|2022-06-13|    KAZ|       1105|\n",
      "|2022-06-13|    CAN|       6368|\n",
      "|2022-06-13|    CHI|       1799|\n",
      "|2022-06-13|    SWE|        664|\n",
      "|2022-06-13|    SRB|      10900|\n",
      "|2022-06-13|    ITA|       9703|\n",
      "|2022-06-13|    FRA|       8604|\n",
      "|2022-06-13|    CZE|       1600|\n",
      "|2022-06-13|    FIN|        945|\n",
      "|2022-06-13|    RUS|      14878|\n",
      "|2022-06-13|    BRA|        621|\n",
      "|2022-06-13|    BLR|       1060|\n",
      "|2022-06-13|    USA|      16663|\n",
      "|2022-06-13|    BUL|       1740|\n",
      "|2022-06-13|    GER|       9573|\n",
      "|2022-06-13|    ESP|      21162|\n",
      "|2022-06-13|    HUN|        932|\n",
      "|2022-06-13|    GBR|       6328|\n",
      "|2022-06-13|    GEO|       1628|\n",
      "|2022-06-13|    POR|        936|\n",
      "|2022-06-13|    GRE|       4945|\n",
      "|2022-06-13|    DEN|       1429|\n",
      "|2022-06-13|    NOR|       5050|\n",
      "|2022-06-13|    AUS|       6949|\n",
      "|2022-06-13|    ARG|       6869|\n",
      "|2022-06-13|    NED|       2278|\n",
      "|2022-06-13|    KOR|        772|\n",
      "|2022-06-13|    BEL|       1080|\n",
      "|2022-06-06|    SRB|      13053|\n",
      "|2022-06-06|    CHI|       2021|\n",
      "|2022-06-06|    JPN|        745|\n",
      "|2022-06-06|    CRO|       2370|\n",
      "|2022-06-06|    GER|      11058|\n",
      "|2022-06-06|    BOL|        733|\n",
      "|2022-06-06|    SUI|       1751|\n",
      "|2022-06-06|    NED|       2392|\n",
      "|2022-06-06|    FRA|       8806|\n",
      "|2022-06-06|    FIN|        945|\n",
      "|2022-06-06|    HUN|        990|\n",
      "|2022-06-06|    LTU|        655|\n",
      "|2022-06-06|    NOR|       5050|\n",
      "|2022-06-06|    BRA|        641|\n",
      "|2022-06-06|    BUL|       1830|\n",
      "|2022-06-06|    RUS|      15303|\n",
      "|2022-06-06|    ITA|      10403|\n",
      "|2022-06-06|    KOR|        844|\n",
      "|2022-06-06|    POR|        936|\n",
      "|2022-06-06|    KAZ|       1108|\n",
      "|2022-06-06|    POL|       4032|\n",
      "|2022-06-06|    GRE|       6100|\n",
      "|2022-06-06|    DEN|       1489|\n",
      "|2022-06-06|    BLR|       1035|\n",
      "|2022-06-06|    USA|      17318|\n",
      "|2022-06-06|    CZE|       1628|\n",
      "|2022-06-06|    SWE|        750|\n",
      "|2022-06-06|    GBR|       5604|\n",
      "|2022-06-06|    CAN|       6486|\n",
      "|2022-06-06|    BEL|       1113|\n",
      "+----------+-------+-----------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.groupby('date', 'country').sum('points').sort('date',ascending=False).show(100)\n",
    "#df_spark.groupby('date', 'country').orderBy(col(\"date\").desc,col(\"sum(points)\").desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa12f3ff",
   "metadata": {},
   "source": [
    "### the transformation I want to implement(for this example I choose date=2022-06-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4144b441",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:15:36.157108Z",
     "start_time": "2022-07-03T21:15:35.720582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+\n",
      "|      date|country|sum(points)|\n",
      "+----------+-------+-----------+\n",
      "|2022-06-20|    ESP|      21829|\n",
      "|2022-06-20|    USA|      17105|\n",
      "|2022-06-20|    RUS|      14841|\n",
      "|2022-06-20|    SRB|      11260|\n",
      "|2022-06-20|    GER|       9693|\n",
      "|2022-06-20|    ITA|       9578|\n",
      "|2022-06-20|    FRA|       7423|\n",
      "|2022-06-20|    AUS|       6941|\n",
      "|2022-06-20|    ARG|       6800|\n",
      "|2022-06-20|    CAN|       6053|\n",
      "|2022-06-20|    GBR|       5396|\n",
      "|2022-06-20|    NOR|       5050|\n",
      "|2022-06-20|    GRE|       4945|\n",
      "|2022-06-20|    POL|       4373|\n",
      "|2022-06-20|    NED|       2477|\n",
      "|2022-06-20|    CRO|       2220|\n",
      "|2022-06-20|    CHI|       1793|\n",
      "|2022-06-20|    BUL|       1785|\n",
      "|2022-06-20|    CZE|       1615|\n",
      "|2022-06-20|    GEO|       1473|\n",
      "+----------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.where(df_spark.date == \"2022-06-20\").groupby('date', 'country').sum('points').sort('sum(points)',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12f62624",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T21:16:30.819932Z",
     "start_time": "2022-07-03T21:16:28.987527Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o163.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 15) (192.168.56.1 executor driver): java.io.IOException: Mkdirs failed to create file:/C:/Users/sokin/\trg_data/_temporary/0/_temporary/attempt_20220704001629708605557850690633_0020_m_000000_15 (exists=false, cwd=file:/C:/Users/sokin)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\r\n\t... 42 more\r\nCaused by: java.io.IOException: Mkdirs failed to create file:/C:/Users/sokin/\trg_data/_temporary/0/_temporary/attempt_20220704001629708605557850690633_0020_m_000000_15 (exists=false, cwd=file:/C:/Users/sokin)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps[:]:\n\u001b[0;32m      3\u001b[0m     _df_spark \u001b[38;5;241m=\u001b[39m df_spark\u001b[38;5;241m.\u001b[39mwhere(df_spark\u001b[38;5;241m.\u001b[39mdate \u001b[38;5;241m==\u001b[39m step[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m     \u001b[43m_df_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43mrg_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trg_env\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1240\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1223\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1224\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1239\u001b[0m )\n\u001b[1;32m-> 1240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trg_env\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trg_env\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trg_env\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o163.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 15) (192.168.56.1 executor driver): java.io.IOException: Mkdirs failed to create file:/C:/Users/sokin/\trg_data/_temporary/0/_temporary/attempt_20220704001629708605557850690633_0020_m_000000_15 (exists=false, cwd=file:/C:/Users/sokin)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\r\n\t... 42 more\r\nCaused by: java.io.IOException: Mkdirs failed to create file:/C:/Users/sokin/\trg_data/_temporary/0/_temporary/attempt_20220704001629708605557850690633_0020_m_000000_15 (exists=false, cwd=file:/C:/Users/sokin)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "steps = df_spark.select(\"date\").distinct().collect()\n",
    "for step in steps[:]:\n",
    "    _df_spark = df_spark.where(df_spark.date == step[0])\n",
    "    _df_spark.coalesce(1).write.mode(\"append\").option(\"header\",\"true\").csv('\\trg_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe5558",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T16:02:43.180307Z",
     "start_time": "2022-07-03T16:02:43.093479Z"
    }
   },
   "outputs": [],
   "source": [
    "step = spark.read.csv('filepath', header=True, inferSchema=True)\n",
    "\n",
    "step.groupby('date', 'country').sum('points').sort('date',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55120b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema = step.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e4952e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c8cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555254cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186ed03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c3889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c3bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
